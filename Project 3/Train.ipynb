{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import custom modules\n",
    "from UNet import UNet, Nested_UNet\n",
    "from utils import load_zipped_pickle, save_zipped_pickle\n",
    "\n",
    "# config device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (original data)\n",
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        low_resolution_x = np.load(\"./Data/low_resolution_x.npy\")\n",
    "        low_resolution_y = np.load(\"./Data/low_resolution_y.npy\")\n",
    "        high_resolution_x = np.load(\"./Data/high_resolution_x.npy\")\n",
    "        high_resolution_y = np.load(\"./Data/high_resolution_y.npy\")\n",
    "\n",
    "        self.train_x = np.concatenate((low_resolution_x, high_resolution_x), axis=0)\n",
    "        self.train_y = np.concatenate((low_resolution_y, high_resolution_y), axis=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x[idx]).float().unsqueeze(0) # 添加channel dim，同时要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "\n",
    "train_dataset = TrainImageDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 105/105 [1:18:16<00:00, 44.73s/it, batch id=23296, loss=2.38e-5] \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet(bilinear)模型 (预计用时: 1h15mins) (batchsize可以设置为128)\n",
    "model = UNet(in_channels=1, out_channels=1, bilinear=True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 105\n",
    "loss_history = []\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/UNet_4_4_bilinear.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 105/105 [1:16:19<00:00, 43.62s/it, batch id=23296, loss=1.15e-5] \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet(ConvTranspose)模型 (预计用时: 1h15mins) (batchsize可以设置为128)\n",
    "model = UNet(in_channels=1, out_channels=1, bilinear=False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 105\n",
    "loss_history = []\n",
    "model.train()\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/UNet_4_4_transpose_conv.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 100/100 [3:41:00<00:00, 132.60s/it, batch id=23296, loss=9.48e-5]  \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet++模型 (预计用时: 1h15mins) (batchsize可以设置为64)\n",
    "model = Nested_UNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "loss_history = []\n",
    "model.train()\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/NestedUNet_4_4.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
