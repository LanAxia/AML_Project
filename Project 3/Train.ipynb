{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import custom modules\n",
    "from UNet import UNet, Nested_UNet\n",
    "from utils import load_zipped_pickle, save_zipped_pickle\n",
    "\n",
    "# config device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (original data)\n",
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        low_resolution_x = np.load(\"./Data/low_resolution_x.npy\")\n",
    "        low_resolution_y = np.load(\"./Data/low_resolution_y.npy\")\n",
    "        high_resolution_x = np.load(\"./Data/high_resolution_x.npy\")\n",
    "        high_resolution_y = np.load(\"./Data/high_resolution_y.npy\")\n",
    "\n",
    "        self.train_x = np.concatenate((low_resolution_x, high_resolution_x), axis=0)\n",
    "        self.train_y = np.concatenate((low_resolution_y, high_resolution_y), axis=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x[idx]).float().unsqueeze(0) # 添加channel dim，同时要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "\n",
    "train_dataset = TrainImageDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (original data) (只有高维数据图像)\n",
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.train_x = np.load(\"./Data/high_resolution_x_256.npy\")\n",
    "        self.train_y = np.load(\"./Data/high_resolution_y_256.npy\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x[idx]).float().unsqueeze(0) # 添加channel dim，同时要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "\n",
    "train_dataset = TrainImageDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (original data) (只有高维数据图像)\n",
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.train_x = np.load(\"./Data/high_resolution_x_256.npy\")\n",
    "        self.train_y = np.load(\"./Data/high_resolution_y_256.npy\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x[idx]).float().unsqueeze(0) # 添加channel dim，同时要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "\n",
    "train_dataset = TrainImageDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (multiple channels data)\n",
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        low_resolution_x = np.load(\"./Data/low_resolution_x_multi_channels.npy\")\n",
    "        low_resolution_y = np.load(\"./Data/low_resolution_y_multi_channels.npy\")\n",
    "        high_resolution_x = np.load(\"./Data/high_resolution_x_multi_channels.npy\")\n",
    "        high_resolution_y = np.load(\"./Data/high_resolution_y_multi_channels.npy\")\n",
    "\n",
    "        self.train_x = np.concatenate((low_resolution_x, high_resolution_x), axis=0)\n",
    "        self.train_y = np.concatenate((low_resolution_y, high_resolution_y), axis=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x[idx]).float()   # 不需要添加channel dim，要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "\n",
    "train_dataset = TrainImageDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files (preprocessed data)\n",
    "class TrainImageDatasetProcessed(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.train_x = np.load(\"./Data/train_x.npy\")\n",
    "        self.train_y = np.load(\"./Data/train_y.npy\")\n",
    "        self.train_x_samples = []\n",
    "        self.train_y_samples = []\n",
    "        for img in self.train_x:\n",
    "            self.train_x_samples += self.sample(img, (256, 256), stride=32)\n",
    "        \n",
    "        for img in self.train_y:\n",
    "            self.train_y_samples += self.sample(img, (256, 256), stride=32)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_x_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = torch.from_numpy(self.train_x_samples[idx]).float().unsqueeze(0) # 添加channel dim，同时要注意转换数据类型\n",
    "        y_i = torch.from_numpy(self.train_y_samples[idx]).float().unsqueeze(0)\n",
    "        return x_i, y_i\n",
    "    \n",
    "    def sample(self, img: np.ndarray, window_size: tuple, stride: int):\n",
    "        samples = []\n",
    "        for i in range(0, img.shape[0] - window_size[0] + 1, stride):\n",
    "            for j in range(0, img.shape[1] - window_size[1] + 1, stride):\n",
    "                samples.append(img[i:i + window_size[0], j:j + window_size[1]])\n",
    "        return samples\n",
    "\n",
    "train_dataset = TrainImageDatasetProcessed()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 100/100 [2:18:59<00:00, 83.39s/it, batch id=39680, loss=1.91e-5]  \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet(bilinear)模型 (预计用时: 1h15mins) (batchsize可以设置为128)\n",
    "model = UNet(in_channels=7, out_channels=1, bilinear=True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "loss_history = []\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device) \n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/UNet_multi_channels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 105/105 [1:16:19<00:00, 43.62s/it, batch id=23296, loss=1.15e-5] \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet(ConvTranspose)模型 (预计用时: 1h15mins) (batchsize可以设置为128)\n",
    "model = UNet(in_channels=1, out_channels=1, bilinear=False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 105\n",
    "loss_history = []\n",
    "model.train()\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/UNet_4_4_transpose_conv.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Training: 100%|██████████| 100/100 [3:41:00<00:00, 132.60s/it, batch id=23296, loss=9.48e-5]  \n"
     ]
    }
   ],
   "source": [
    "# 训练UNet++模型 (预计用时: 1h15mins) (batchsize可以设置为64)\n",
    "model = Nested_UNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # 不需要单独计算sigmoid，最后在预测的时候需要用sigmoid\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "loss_history = []\n",
    "model.train()\n",
    "with trange(epochs, desc=\"Model Training\") as t:\n",
    "    postfix = {}\n",
    "    for epoch in t:\n",
    "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            check_loss = loss.to(cpu_device).detach().item()\n",
    "            loss_history.append(check_loss)\n",
    "            postfix[\"batch id\"] = (batch_i + 1) * train_dataloader.batch_size\n",
    "            postfix[\"loss\"] = check_loss\n",
    "            t.set_postfix(postfix)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"./Model/NestedUNet_4_4.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
