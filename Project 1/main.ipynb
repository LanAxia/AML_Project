{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries (run)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, r_regression, f_regression\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, CompoundKernel, Product, Sum, ExpSineSquared, RationalQuadratic\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# outlier\n",
    "from sklearn.base import OutlierMixin\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "import lightgbm as lgb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Linear, Dropout\n",
    "from torch.nn.functional import tanh, softmax, mse_loss, relu, sigmoid\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# load and split data (run)\n",
    "data_X_train = pd.read_csv('Data/X_train.csv', header=0, index_col=0)\n",
    "data_y_train = pd.read_csv('Data/y_train.csv', header=0, index_col=0)\n",
    "data_X_test = pd.read_csv('Data/X_test.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# data info (run)\n",
    "data_X_train.describe()\n",
    "\"\"\"\n",
    "Data Shape: 1212 x 832\n",
    "Data Lost: a lot\n",
    "data scale: large\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# transfer data to numpy (run)\n",
    "X_train = data_X_train.to_numpy()\n",
    "y_train = data_y_train.to_numpy()\n",
    "X_test = data_X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理缺省值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# KNN Imputer (run)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# scalar (run)\n",
    "x_scalar = RobustScaler()\n",
    "X_train = x_scalar.fit_transform(X_train)\n",
    "X_test = x_scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 删除变化过小的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# scalar (run)\n",
    "del_columns_id_all0 = np.where(X_train.sum(axis=0) == 0)\n",
    "X_train = np.delete(X_train, del_columns_id_all0, axis=1)\n",
    "X_test = np.delete(X_test, del_columns_id_all0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保留特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# 使用selectkbest方法选择特征 (run)\n",
    "skb = SelectKBest(f_regression, k=200)\n",
    "X_train = skb.fit_transform(X_train, y_train.ravel())\n",
    "X_test = skb.transform(X_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### 噪声探测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的噪声检测方法只适合用于验证，不适合用于最终的模型，都不需要运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# EllipticEnvelope\n",
    "outlier_detector = EllipticEnvelope()\n",
    "outlier_ids = outlier_detector.fit_predict(X_train, y_train)\n",
    "non_outlier_ids = np.where(outlier_ids != -1)\n",
    "X_train = X_train[non_outlier_ids]\n",
    "y_train = y_train[non_outlier_ids]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# LocalOutlierFactor\n",
    "outlier_detector = LocalOutlierFactor(n_neighbors=3)\n",
    "outlier_ids = outlier_detector.fit_predict(X_train, y_train)\n",
    "non_outlier_ids = np.where(outlier_ids != -1)\n",
    "X_train = X_train[non_outlier_ids]\n",
    "y_train = y_train[non_outlier_ids]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# OneClassSVM\n",
    "outlier_detector = OneClassSVM(kernel=\"linear\")\n",
    "outlier_ids = outlier_detector.fit_predict(X_train, y_train)\n",
    "non_outlier_ids = np.where(outlier_ids != -1)\n",
    "X_train = X_train[non_outlier_ids]\n",
    "y_train = y_train[non_outlier_ids]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Regressor (Matern)\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    param = {'alpha': 1e-09, 'kernel': Matern(length_scale=0.5, nu=1.5) + RBF(length_scale=1)}\n",
    "    model = GaussianProcessRegressor(**param)\n",
    "    model.fit(fold_X_train, fold_y_train)\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV\n",
    "model = GaussianProcessRegressor(random_state=0)\n",
    "params = {\n",
    "    \"kernel\": [Matern(nu=1.5), Matern(nu=0.5), Matern(nu=2.5), Sum(Matern(nu=1.5), Matern(nu=0.5)), Sum(Matern(nu=1.5), RBF()), Sum(Matern(nu=0.5), RBF()), Sum(Matern(nu=2.5), RBF()), ExpSineSquared(), RationalQuadratic()], \n",
    "    \"alpha\": [1e-10, 2e-10, 5e-10, 1e-9, 2e-9, 5e-9, 1e-8, 2e-8, 5e-8], \n",
    "}\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "regressor = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring=scorer)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Regressor (RBF)\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = GaussianProcessRegressor(kernel=RBF(length_scale=10), random_state=0)\n",
    "    model.fit(fold_X_train, fold_y_train)\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boost Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest Regressor\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = IsolationForest(n_estimators=150, random_state=0)\n",
    "    model.fit(fold_X_train, fold_y_train)\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "    model.fit(fold_X_train, fold_y_train.ravel())\n",
    "    fold_y_pred = np.round(model.predict(fold_X_valid))\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Regressor\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = AdaBoostRegressor(n_estimators=100, learning_rate=0.1, loss=\"square\")\n",
    "    model.fit(fold_X_train, fold_y_train.ravel())\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = xgb.XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.11, n_jobs=20)\n",
    "    model.fit(fold_X_train, fold_y_train.ravel())\n",
    "    fold_y_pred = np.round(model.predict(fold_X_valid))\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost(最优)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost (run)\n",
    "# 模型：换loss function\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    param = {\n",
    "        'iterations': 1500,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.6,\n",
    "        'rsm': 0.6, \n",
    "        \"max_depth\": 5, \n",
    "    }\n",
    "    model = cat.CatBoostRegressor(**param)\n",
    "    model.fit(fold_X_train, fold_y_train.ravel())\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV\n",
    "model = cat.CatBoostRegressor()\n",
    "params = {\n",
    "    \"kernel\": [Matern(nu=1.5), Matern(nu=0.5), Matern(nu=2.5), RBF(), Matern(nu=1.5) + Matern(nu=0.5), Matern(nu=1.5) + RBF(), Matern(nu=0.5) + RBF()], \n",
    "    \"alpha\": [1e-11, 2e-11, 5e-11, 1e-10, 2e-10, 5e-10, 1e-9, 2e-9, 5e-9, 1e-8, 2e-8, 5e-8], \n",
    "}\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "regressor = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring=scorer)\n",
    "regressor.fit(X_train, y_train)\n",
    "regressor.cv_results_[\"rank_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "fold_num = 5\n",
    "\n",
    "kf = KFold(n_splits=fold_num)\n",
    "fold_scores = []\n",
    "for i, (train_ids, valid_ids) in enumerate(kf.split(X_train)):\n",
    "    # split validation data\n",
    "    fold_X_train = X_train[train_ids]\n",
    "    fold_y_train = y_train[train_ids]\n",
    "    fold_X_valid = X_train[valid_ids]\n",
    "    fold_y_valid = y_train[valid_ids]\n",
    "\n",
    "    # train model\n",
    "    model = lgb.LGBMRegressor(n_estimators=50)\n",
    "    model.fit(fold_X_train, fold_y_train.ravel())\n",
    "    fold_y_pred = model.predict(fold_X_valid)\n",
    "\n",
    "    # calculate score\n",
    "    fold_scores.append(r2_score(fold_y_valid, fold_y_pred))\n",
    "fold_score = np.average(fold_scores)\n",
    "print(fold_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合模型(最终使用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixModel(object):\n",
    "    def __init__(self):\n",
    "        # 尽量选一些性能表现比较好的model\n",
    "        self.cat = cat.CatBoostRegressor(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "        self.xgb = xgb.XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.05, n_jobs=20)\n",
    "        self.lgbm = lgb.LGBMRegressor(n_estimators=150)\n",
    "        self.gp1 = GaussianProcessRegressor(alpha=1e-09, kernel=Sum(Matern(length_scale=0.5, nu=1.5), RBF(length_scale=1)))\n",
    "        self.gp2 = GaussianProcessRegressor(alpha=1e-09, kernel=Matern(length_scale=0.5, nu=0.5))\n",
    "        self.gp3 = GaussianProcessRegressor(alpha=1e-09, kernel=RBF(length_scale=1))\n",
    "        self.lr = LinearRegression()\n",
    "        self.isf = IsolationForest(n_estimators=150, random_state=0)\n",
    "        self.gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr, self.isf, self.gbr]\n",
    "        # self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr, self.isf, self.gbr]\n",
    "        # self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr]\n",
    "        self.intermediate_prediction = []\n",
    "\n",
    "        self.mix_model = cat.CatBoostRegressor(iterations=1500, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "    \n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        for model in self.basic_models:\n",
    "            model.fit(X, y)\n",
    "            self.intermediate_prediction.append(model.predict(X).reshape((-1, 1)))\n",
    "        \n",
    "        self.intermediate_prediction = np.concatenate(self.intermediate_prediction, axis=1)\n",
    "\n",
    "        self.mix_model.fit(self.intermediate_prediction, y)\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        intermediate_prediction = []\n",
    "        for model in self.basic_models:\n",
    "            intermediate_prediction.append(model.predict(X).reshape((-1, 1)))\n",
    "        intermediate_prediction = np.concatenate(intermediate_prediction, axis=1)\n",
    "        return self.mix_model.predict(intermediate_prediction)\n",
    "\n",
    "# 最终使用该模型\n",
    "class MixModelCL(object):\n",
    "    def __init__(self):\n",
    "        # 尽量选一些性能表现比较好的model\n",
    "        self.cat = cat.CatBoostRegressor(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "        self.xgb = xgb.XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.05, n_jobs=20)\n",
    "        self.lgbm = lgb.LGBMRegressor(n_estimators=150)\n",
    "        self.gp1 = GaussianProcessRegressor(alpha=1e-09, kernel=Sum(Matern(length_scale=0.5, nu=1.5), RBF(length_scale=1)))\n",
    "        self.gp2 = GaussianProcessRegressor(alpha=1e-09, kernel=Matern(length_scale=0.5, nu=0.5))\n",
    "        self.gp3 = GaussianProcessRegressor(alpha=1e-09, kernel=RBF(length_scale=1))\n",
    "        self.lr = LinearRegression()\n",
    "        self.isf = IsolationForest(n_estimators=150, random_state=0)\n",
    "        self.gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr, self.isf, self.gbr]\n",
    "        # self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr, self.isf, self.gbr]\n",
    "        # self.basic_models = [self.cat, self.xgb, self.lgbm, self.gp1, self.gp2, self.gp3, self.lr]\n",
    "        self.intermediate_prediction = []\n",
    "\n",
    "        self.mix_model = cat.CatBoostRegressor(iterations=1500, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "\n",
    "        self.high_model = cat.CatBoostRegressor(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "        self.low_model = cat.CatBoostRegressor(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "\n",
    "        self.classify_high_model = cat.CatBoostClassifier(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "        self.classify_low_model = cat.CatBoostClassifier(iterations=2000, learning_rate=0.05, subsample=0.6, rsm=0.6, max_depth=5)\n",
    "    \n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        for model in self.basic_models:\n",
    "            model.fit(X, y)\n",
    "            self.intermediate_prediction.append(model.predict(X).reshape((-1, 1)))\n",
    "        \n",
    "        self.intermediate_prediction = np.concatenate(self.intermediate_prediction, axis=1)\n",
    "\n",
    "        self.mix_model.fit(self.intermediate_prediction, y)\n",
    "\n",
    "        # process imbalanced data\n",
    "        high_ids = np.where(y >= 80)\n",
    "        low_ids = np.where(y <= 50)\n",
    "        high_y_ones = (y >= 80).astype(int)\n",
    "        low_y_ones = (y <= 50).astype(int)\n",
    "        self.classify_high_model.fit(X, high_y_ones)\n",
    "        self.classify_low_model.fit(X, low_y_ones)\n",
    "\n",
    "        self.high_model.fit(X[high_ids], y[high_ids])\n",
    "        self.low_model.fit(X[low_ids], y[low_ids])\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        intermediate_prediction = []\n",
    "        for model in self.basic_models:\n",
    "            intermediate_prediction.append(model.predict(X).reshape((-1, 1)))\n",
    "        intermediate_prediction = np.concatenate(intermediate_prediction, axis=1)\n",
    "        pred = self.mix_model.predict(intermediate_prediction)\n",
    "\n",
    "        # process imbalanced data\n",
    "        high_id_mask = self.classify_high_model.predict(X)\n",
    "        low_id_mask = self.classify_low_model.predict(X) == 1\n",
    "        cooccur_mask = high_id_mask * low_id_mask\n",
    "        high_id_mask = high_id_mask - cooccur_mask\n",
    "        low_id_mask = low_id_mask - cooccur_mask\n",
    "\n",
    "        high_pred = self.high_model.predict(X) * high_id_mask\n",
    "        low_pred = self.low_model.predict(X) * low_id_mask\n",
    "        cooccur_mask = high_id_mask * low_id_mask\n",
    "\n",
    "        result = (pred + high_pred + low_pred + pred * (1 - high_id_mask) * (1 - low_id_mask)) / 2\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用mix model进行预测\n",
    "model = MixModel()\n",
    "model.fit(X_train, y_train.ravel())\n",
    "# y_pred = np.round(model.predict(X_test))\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"y\"], index=data_X_test.index).reset_index()\n",
    "y_pred_df[\"id\"] = y_pred_df[\"id\"].astype(int)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用mix model with classifier进行预测\n",
    "model = MixModelCL()\n",
    "model.fit(X_train, y_train.ravel())\n",
    "# y_pred = np.round(model.predict(X_test))\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"y\"], index=data_X_test.index).reset_index()\n",
    "y_pred_df[\"id\"] = y_pred_df[\"id\"].astype(int)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用catboost进行预测\n",
    "param = {\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.6,\n",
    "        'rsm': 0.6, \n",
    "        \"max_depth\": 5, \n",
    "        \"iterations\": 2000,\n",
    "    }\n",
    "model = cat.CatBoostRegressor(**param)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "# y_pred = np.round(model.predict(X_test))\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"y\"], index=data_X_test.index).reset_index()\n",
    "y_pred_df[\"id\"] = y_pred_df[\"id\"].astype(int)\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.to_csv(\"mix_model_withround.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
