{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, r_regression, f_regression\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, CompoundKernel, Product, Sum, ExpSineSquared, RationalQuadratic\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, make_scorer, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# boost algorithm\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "import lightgbm as lgb\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Linear, Dropout\n",
    "from torch.nn.functional import tanh, softmax, mse_loss, relu, sigmoid, binary_cross_entropy, nll_loss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# bio library\n",
    "import biosppy\n",
    "from biosppy import storage\n",
    "from biosppy.signals import ecg\n",
    "\n",
    "DATA_DIR = \"Data\"\n",
    "RESULT_DIR = \"Result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X_train_df = pd.read_csv(os.path.join(DATA_DIR, \"X_train.csv\"), header=0, index_col=0)\n",
    "X_test_df = pd.read_csv(os.path.join(DATA_DIR, \"X_test.csv\"), header=0, index_col=0)\n",
    "y_train_df = pd.read_csv(os.path.join(DATA_DIR, \"y_train.csv\"), header=0, index_col=0)\n",
    "\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取有效长度\n",
    "X_train_len = []\n",
    "for row in X_train:\n",
    "    tail_id = np.where(np.isnan(row))[0]\n",
    "    if tail_id.shape[0] > 0:\n",
    "        X_train_len.append(tail_id[0])\n",
    "    else:\n",
    "        X_train_len.append(X_train.shape[1])\n",
    "\n",
    "X_test_len = []\n",
    "for row in X_test:\n",
    "    tail_id = np.where(np.isnan(row))[0]\n",
    "    if tail_id.shape[0] > 0:\n",
    "        X_test_len.append(tail_id[0])\n",
    "    else:\n",
    "        X_test_len.append(X_test.shape[1])\n",
    "\n",
    "X_train_len, X_test_len = np.array(X_train_len), np.array(X_test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Valid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ecg info (比较松)\n",
    "def check_template_result(templates: np.ndarray) -> list:\n",
    "    # 检查有没有一个template里有多个心跳的情况\n",
    "    check_result = True\n",
    "    error_num = 0\n",
    "    error_ids = []\n",
    "    for template_i, template in enumerate(templates):\n",
    "        peak_threshold = np.max(template) * 0.7\n",
    "        peak_region = np.array(np.where(template > peak_threshold))\n",
    "        if np.max(peak_region) - np.min(peak_region) > 0.5 * template.shape[0]:\n",
    "            error_num += 1\n",
    "            error_ids.append(template_i)\n",
    "    return error_ids\n",
    "\n",
    "def get_ecg_info(X, X_len):\n",
    "    ts_lst = []\n",
    "    filtered_lst = []\n",
    "    rpeaks_lst = []\n",
    "    templates_ts_lst = []\n",
    "    templates_lst = []\n",
    "    heart_rate_ts_lst = []\n",
    "    heart_rate_lst = []\n",
    "\n",
    "    error_ids = []\n",
    "    part_error_lst = []\n",
    "    for i, (signal, sig_len) in enumerate(zip(X, X_len)):\n",
    "        ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal[:sig_len], sampling_rate=300., show=False)\n",
    "        # check template\n",
    "        # if check_ecg_result(templates) == False:\n",
    "        #     error_ids.append(i)\n",
    "        #     error_templates.append(templates)\n",
    "        #     continue\n",
    "\n",
    "        # template_error_ids = check_template_result(templates) # 以较轻松的方式处理ecg处理异常\n",
    "        template_error_ids = [] # 以较轻松的方式处理ecg处理异常\n",
    "\n",
    "        # delete error data\n",
    "        rpeaks = np.delete(rpeaks, template_error_ids, axis=0)\n",
    "        templates_ts = np.delete(templates_ts, template_error_ids, axis=0)\n",
    "        templates = np.delete(templates, template_error_ids, axis=0)\n",
    "        if len(templates) < 1:\n",
    "            error_ids.append(i)\n",
    "            continue\n",
    "\n",
    "        if len(template_error_ids) > 0:\n",
    "            part_error_lst.append(len(ts_lst))\n",
    "\n",
    "        ts_lst.append(ts) # Signal time axis reference (seconds)\n",
    "        filtered_lst.append(filtered) # Filtered ECG signal\n",
    "        rpeaks_lst.append(rpeaks) # R-peak location indices\n",
    "        templates_ts_lst.append(templates_ts) # Templates time axis reference\n",
    "        templates_lst.append(templates) # Extracted heartbeat templates\n",
    "        heart_rate_ts_lst.append(heart_rate_ts) # Heart rate time axis reference (seconds)\n",
    "        heart_rate_lst.append(heart_rate) # Instantaneous heart rate (bpm)\n",
    "    return ts_lst, filtered_lst, rpeaks_lst, templates_ts_lst, templates_lst, heart_rate_ts_lst, heart_rate_lst\n",
    "\n",
    "ts_lst, filtered_lst, rpeaks_lst, templates_ts_lst, templates_lst, heart_rate_ts_lst, heart_rate_lst = get_ecg_info(X_train, X_train_len)\n",
    "ts_lst_test, filtered_lst_test, rpeaks_lst_test, templates_ts_lst_test, templates_lst_test, heart_rate_ts_lst_test, heart_rate_lst_test = get_ecg_info(X_test, X_test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average template\n",
    "max_height = None\n",
    "for templates in templates_lst:\n",
    "    for template in templates:\n",
    "        if max_height is None or np.max(template) > max_height:\n",
    "            max_height = np.max(template)\n",
    "\n",
    "# scaler现在只是简单的缩放，不确定绝对高度有没有用\n",
    "def scaler(template: np.array):\n",
    "    result = template / max_height\n",
    "    return result\n",
    "\n",
    "# 对所有的templates进行缩放\n",
    "templates_lst = [scaler(templates) for templates in templates_lst]\n",
    "\n",
    "def get_average_templates(templates):\n",
    "    templates = scaler(templates)\n",
    "    avg_templates = templates.sum(axis=0) / templates.shape[0]\n",
    "    return avg_templates\n",
    "\n",
    "avg_templates_lst = [get_average_templates(templates) for templates in templates_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PQRST_from_template(template: np.array):\n",
    "    error_state = False\n",
    "\n",
    "    # get R\n",
    "    R_id = np.where(template == np.max(template))[0][0]\n",
    "    R = template[R_id]\n",
    "\n",
    "    # get Q\n",
    "    if R_id == 0:\n",
    "        Q_id = R_id\n",
    "        error_state = True\n",
    "    else:\n",
    "        Q_id = np.where(template[:R_id] == np.min(template[:R_id]))[0][0]\n",
    "    Q = template[Q_id]\n",
    "\n",
    "    # get P\n",
    "    if Q_id == 0:\n",
    "        P_id = 0\n",
    "        error_state = True\n",
    "    else:\n",
    "        P_id = np.where(template[:Q_id] == np.max(template[:Q_id]))[0][0]\n",
    "    P = template[P_id]\n",
    "\n",
    "    # get S\n",
    "    if R_id == 179:\n",
    "        S_id = R_id\n",
    "        error_state = True\n",
    "    else:\n",
    "        S_id = np.where(template[R_id + 1:] == np.min(template[R_id + 1:]))[0][0] + R_id + 1\n",
    "    S = template[S_id]\n",
    "\n",
    "    # get T\n",
    "    if S_id == 179:\n",
    "        T_id = 179\n",
    "        error_state = True\n",
    "    else:\n",
    "        T_id = np.where(template[S_id + 1:] == np.max(template[S_id + 1:]))[0][0] + S_id + 1\n",
    "    T = template[T_id]\n",
    "\n",
    "    # assert (P_id < Q_id and Q_id < R_id and R_id < S_id and S_id < T_id)\n",
    "\n",
    "    # cal interval\n",
    "    QRS = S_id - Q_id\n",
    "    PR = R_id - P_id\n",
    "    PQ = R_id - Q_id\n",
    "    ST = T_id - S_id\n",
    "    QT = T_id - Q_id\n",
    "    return (P, Q, R, S, T), (P_id, Q_id, R_id, S_id, T_id), (QRS, PR, PQ, ST, QT), error_state\n",
    "\n",
    "# get P Q R S T\n",
    "def get_PQRST(templates_lst):\n",
    "    PQRST = []\n",
    "    for templates_i, templates in enumerate(templates_lst):\n",
    "        template_PQRST = {\n",
    "            \"P\": [], \n",
    "            \"Q\": [], \n",
    "            \"R\": [], \n",
    "            \"S\": [], \n",
    "            \"T\": [], \n",
    "            \"P_id\": [], \n",
    "            \"Q_id\": [], \n",
    "            \"R_id\": [], \n",
    "            \"S_id\": [], \n",
    "            \"T_id\": [], \n",
    "            \"QRS\": [], \n",
    "            \"PR\": [], \n",
    "            \"PQ\": [], \n",
    "            \"ST\": [], \n",
    "            \"QT\": [], \n",
    "            \"error_state\": []\n",
    "        }\n",
    "        for template_i, template in enumerate(templates):\n",
    "            (P, Q, R, S, T), (P_id, Q_id, R_id, S_id, T_id), (QRS, PR, PQ, ST, QT), error_state = get_PQRST_from_template(template)\n",
    "            template_PQRST[\"P\"].append(P)\n",
    "            template_PQRST[\"Q\"].append(Q)\n",
    "            template_PQRST[\"R\"].append(R)\n",
    "            template_PQRST[\"S\"].append(S)\n",
    "            template_PQRST[\"T\"].append(T)\n",
    "            template_PQRST[\"P_id\"].append(P_id)\n",
    "            template_PQRST[\"Q_id\"].append(Q_id)\n",
    "            template_PQRST[\"R_id\"].append(R_id)\n",
    "            template_PQRST[\"S_id\"].append(S_id)\n",
    "            template_PQRST[\"T_id\"].append(T_id)\n",
    "            template_PQRST[\"QRS\"].append(QRS)\n",
    "            template_PQRST[\"PR\"].append(PR)\n",
    "            template_PQRST[\"PQ\"].append(PQ)\n",
    "            template_PQRST[\"ST\"].append(ST)\n",
    "            template_PQRST[\"QT\"].append(QT)\n",
    "            template_PQRST[\"error_state\"].append(error_state)\n",
    "        PQRST.append(template_PQRST)\n",
    "    return PQRST\n",
    "PQRST = get_PQRST(templates_lst)\n",
    "PQRST_test = get_PQRST(templates_lst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理rpeak\n",
    "rpeaks_new = []\n",
    "for rpeaks in rpeaks_lst:\n",
    "    rpeaks_iterval = []\n",
    "    for i in range(1, rpeaks.shape[0]):\n",
    "        rpeaks_iterval.append(rpeaks[i] - rpeaks[i - 1])\n",
    "    rpeaks_iterval = np.array(rpeaks_iterval)\n",
    "    rpeaks_new.append(rpeaks_iterval)\n",
    "\n",
    "rpeaks_new_test = []\n",
    "for rpeaks in rpeaks_lst_test:\n",
    "    rpeaks_iterval = []\n",
    "    for i in range(1, rpeaks.shape[0]):\n",
    "        rpeaks_iterval.append(rpeaks[i] - rpeaks[i - 1])\n",
    "    rpeaks_iterval = np.array(rpeaks_iterval)\n",
    "    rpeaks_new_test.append(rpeaks_iterval)\n",
    "\n",
    "# 处理heart_rate\n",
    "heart_rate_new = []\n",
    "for heart_rate in heart_rate_lst:\n",
    "    if heart_rate.shape[0] == 0:\n",
    "        heart_rate = np.array(-100)\n",
    "    heart_rate_new.append(heart_rate)\n",
    "\n",
    "heart_rate_new_test = []\n",
    "for heart_rate in heart_rate_lst_test:\n",
    "    if heart_rate.shape[0] == 0:\n",
    "        heart_rate = np.array(-100)\n",
    "    heart_rate_new_test.append(heart_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PQRST_scope_stats(templates_lst: [np.ndarray], start_ids: [np.ndarray], end_ids: [np.ndarray]) -> np.ndarray:\n",
    "    scope_mean = []\n",
    "    scope_std = []\n",
    "    scope_max = []\n",
    "    scope_min = []\n",
    "    for templates, start_id, end_id in zip(templates_lst, start_ids, end_ids):\n",
    "        templates = templates / np.max(templates) # 归一化\n",
    "        templates_mean = []\n",
    "        templates_std = []\n",
    "        templates_max = []\n",
    "        templates_min = []\n",
    "        for template, s_i, e_i in zip(templates, start_id, end_id):\n",
    "            template = template[s_i: e_i + 1]\n",
    "            templates_mean.append(np.mean(template))\n",
    "            templates_std.append(np.std(template))\n",
    "            templates_max.append(np.max(template))\n",
    "            templates_min.append(np.min(template))\n",
    "        scope_mean.append(np.mean(templates_mean))\n",
    "        scope_std.append(np.mean(templates_std))\n",
    "        scope_max.append(np.mean(templates_max))\n",
    "        scope_min.append(np.mean(templates_min))\n",
    "    scope_mean = np.array(scope_mean)\n",
    "    scope_std = np.array(scope_std)\n",
    "    scope_max = np.array(scope_max)\n",
    "    scope_min = np.array(scope_min)\n",
    "    return scope_mean, scope_std, scope_max, scope_min\n",
    "\n",
    "def get_PQRST_range_stats(start_ids: [np.ndarray], end_ids: [np.ndarray]) -> np.ndarray:\n",
    "    range_mean = []\n",
    "    range_std = []\n",
    "    range_max = []\n",
    "    range_min = []\n",
    "    for start_id, end_id in zip(start_ids, end_ids):\n",
    "        diff_id = np.array(end_id) - np.array(start_id)\n",
    "        templates_mean = np.mean(diff_id)\n",
    "        templates_std = np.std(diff_id)\n",
    "        templates_max = np.max(diff_id)\n",
    "        templates_min = np.min(diff_id)\n",
    "        range_mean.append(np.mean(templates_mean))\n",
    "        range_std.append(np.mean(templates_std))\n",
    "        range_max.append(np.mean(templates_max))\n",
    "        range_min.append(np.mean(templates_min))\n",
    "    range_mean = np.array(range_mean)\n",
    "    range_std = np.array(range_std)\n",
    "    range_max = np.array(range_max)\n",
    "    range_min = np.array(range_min)\n",
    "    return range_mean, range_std, range_max, range_min\n",
    "\n",
    "def get_PQRST_slope_stats(templates_lst: [np.ndarray], start_ids: [np.ndarray], end_ids: [np.ndarray]) -> np.ndarray:\n",
    "    slope_mean = []\n",
    "    slope_std = []\n",
    "    slope_max = []\n",
    "    slope_min = []\n",
    "    for templates, start_id, end_id in zip(templates_lst, start_ids, end_ids):\n",
    "        templates = templates / np.max(templates) # 归一化\n",
    "        slopes = []\n",
    "        for template, s_i, e_i in zip(templates, start_id, end_id):\n",
    "            if s_i == e_i:\n",
    "                slope = 0\n",
    "            else:\n",
    "                slope = (template[e_i] - template[s_i]) / (e_i - s_i)\n",
    "            slopes.append(slope)\n",
    "        slope_mean.append(np.mean(slopes))\n",
    "        slope_std.append(np.std(slopes))\n",
    "        slope_max.append(np.max(slopes))\n",
    "        slope_min.append(np.min(slopes))\n",
    "    slope_mean = np.array(slope_mean)\n",
    "    slope_std = np.array(slope_std)\n",
    "    slope_max = np.array(slope_max)\n",
    "    slope_min = np.array(slope_min)\n",
    "    return slope_mean, slope_std, slope_max, slope_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计各种指标\n",
    "def get_valid_features(rpeaks, heart_rate, PQRST, rpeaks_ids, filtered_ecg, templates_lst):\n",
    "    # rpeak\n",
    "    rpeak_mean = np.array([np.average(x) for x in rpeaks])\n",
    "    rpeak_median = np.array([np.median(x) for x in rpeaks])\n",
    "    rpeak_std = np.array([np.std(x) for x in rpeaks])\n",
    "    rpeak_min = np.array([np.min(x) for x in rpeaks])\n",
    "    rpeak_max = np.array([np.max(x) for x in rpeaks])\n",
    "\n",
    "    # heart rate\n",
    "    hr_mean = np.array([np.average(x) for x in heart_rate])\n",
    "    hr_median = np.array([np.median(x) for x in heart_rate])\n",
    "    hr_std = np.array([np.std(x) for x in heart_rate])\n",
    "    hr_min = np.array([np.min(x) for x in heart_rate])\n",
    "    hr_max = np.array([np.max(x) for x in heart_rate])\n",
    "\n",
    "    # PQRST\n",
    "    # P\n",
    "    P_mean = np.array([np.mean(x[\"P\"]) for x in PQRST])\n",
    "    P_median = np.array([np.median(x[\"P\"]) for x in PQRST])\n",
    "    P_std = np.array([np.std(x[\"P\"]) for x in PQRST])\n",
    "    P_min = np.array([np.min(x[\"P\"]) for x in PQRST])\n",
    "    P_max = np.array([np.max(x[\"P\"]) for x in PQRST])\n",
    "\n",
    "    # Q\n",
    "    Q_mean = np.array([np.mean(x[\"Q\"]) for x in PQRST])\n",
    "    Q_median = np.array([np.median(x[\"Q\"]) for x in PQRST])\n",
    "    Q_std = np.array([np.std(x[\"Q\"]) for x in PQRST])\n",
    "    Q_min = np.array([np.min(x[\"Q\"]) for x in PQRST])\n",
    "    Q_max = np.array([np.max(x[\"Q\"]) for x in PQRST])\n",
    "\n",
    "    # R\n",
    "    R_mean = np.array([np.mean(x[\"R\"]) for x in PQRST])\n",
    "    R_median = np.array([np.median(x[\"R\"]) for x in PQRST])\n",
    "    R_std = np.array([np.std(x[\"R\"]) for x in PQRST])\n",
    "    R_min = np.array([np.min(x[\"R\"]) for x in PQRST])\n",
    "    R_max = np.array([np.max(x[\"R\"]) for x in PQRST])\n",
    "\n",
    "    # S\n",
    "    S_mean = np.array([np.mean(x[\"S\"]) for x in PQRST])\n",
    "    S_median = np.array([np.median(x[\"S\"]) for x in PQRST])\n",
    "    S_std = np.array([np.std(x[\"S\"]) for x in PQRST])\n",
    "    S_min = np.array([np.min(x[\"S\"]) for x in PQRST])\n",
    "    S_max = np.array([np.max(x[\"S\"]) for x in PQRST])\n",
    "\n",
    "    # T\n",
    "    T_mean = np.array([np.mean(x[\"T\"]) for x in PQRST])\n",
    "    T_median = np.array([np.median(x[\"T\"]) for x in PQRST])\n",
    "    T_std = np.array([np.std(x[\"T\"]) for x in PQRST])\n",
    "    T_min = np.array([np.min(x[\"T\"]) for x in PQRST])\n",
    "    T_max = np.array([np.max(x[\"T\"]) for x in PQRST])\n",
    "\n",
    "    # P_i\n",
    "    P_id_mean = np.array([np.mean(x[\"P_id\"]) for x in PQRST])\n",
    "    P_id_median = np.array([np.median(x[\"P_id\"]) for x in PQRST])\n",
    "    P_id_std = np.array([np.std(x[\"P_id\"]) for x in PQRST])\n",
    "    P_id_min = np.array([np.min(x[\"P_id\"]) for x in PQRST])\n",
    "    P_id_max = np.array([np.max(x[\"P_id\"]) for x in PQRST])\n",
    "\n",
    "    # Q_i\n",
    "    Q_id_mean = np.array([np.mean(x[\"Q_id\"]) for x in PQRST])\n",
    "    Q_id_median = np.array([np.median(x[\"Q_id\"]) for x in PQRST])\n",
    "    Q_id_std = np.array([np.std(x[\"Q_id\"]) for x in PQRST])\n",
    "    Q_id_min = np.array([np.min(x[\"Q_id\"]) for x in PQRST])\n",
    "    Q_id_max = np.array([np.max(x[\"Q_id\"]) for x in PQRST])\n",
    "\n",
    "    # R_i\n",
    "    R_id_mean = np.array([np.mean(x[\"R_id\"]) for x in PQRST])\n",
    "    R_id_median = np.array([np.median(x[\"R_id\"]) for x in PQRST])\n",
    "    R_id_std = np.array([np.std(x[\"R_id\"]) for x in PQRST])\n",
    "    R_id_min = np.array([np.min(x[\"R_id\"]) for x in PQRST])\n",
    "    R_id_max = np.array([np.max(x[\"R_id\"]) for x in PQRST])\n",
    "\n",
    "    # S_i\n",
    "    S_id_mean = np.array([np.mean(x[\"S_id\"]) for x in PQRST])\n",
    "    S_id_median = np.array([np.median(x[\"S_id\"]) for x in PQRST])\n",
    "    S_id_std = np.array([np.std(x[\"S_id\"]) for x in PQRST])\n",
    "    S_id_min = np.array([np.min(x[\"S_id\"]) for x in PQRST])\n",
    "    S_id_max = np.array([np.max(x[\"S_id\"]) for x in PQRST])\n",
    "\n",
    "    # T_i\n",
    "    T_id_mean = np.array([np.mean(x[\"T_id\"]) for x in PQRST])\n",
    "    T_id_median = np.array([np.median(x[\"T_id\"]) for x in PQRST])\n",
    "    T_id_std = np.array([np.std(x[\"T_id\"]) for x in PQRST])\n",
    "    T_id_min = np.array([np.min(x[\"T_id\"]) for x in PQRST])\n",
    "    T_id_max = np.array([np.max(x[\"T_id\"]) for x in PQRST])\n",
    "\n",
    "    # QRS\n",
    "    QRS_mean = np.array([np.mean(x[\"QRS\"]) for x in PQRST])\n",
    "    QRS_median = np.array([np.median(x[\"QRS\"]) for x in PQRST])\n",
    "    QRS_std = np.array([np.std(x[\"QRS\"]) for x in PQRST])\n",
    "    QRS_min = np.array([np.min(x[\"QRS\"]) for x in PQRST])\n",
    "    QRS_max = np.array([np.max(x[\"QRS\"]) for x in PQRST])\n",
    "\n",
    "    # PR\n",
    "    PR_mean = np.array([np.mean(x[\"PR\"]) for x in PQRST])\n",
    "    PR_median = np.array([np.median(x[\"PR\"]) for x in PQRST])\n",
    "    PR_std = np.array([np.std(x[\"PR\"]) for x in PQRST])\n",
    "    PR_min = np.array([np.min(x[\"PR\"]) for x in PQRST])\n",
    "    PR_max = np.array([np.max(x[\"PR\"]) for x in PQRST])\n",
    "\n",
    "    # PQ\n",
    "    PQ_mean = np.array([np.mean(x[\"PQ\"]) for x in PQRST])\n",
    "    PQ_median = np.array([np.median(x[\"PQ\"]) for x in PQRST])\n",
    "    PQ_std = np.array([np.std(x[\"PQ\"]) for x in PQRST])\n",
    "    PQ_min = np.array([np.min(x[\"PQ\"]) for x in PQRST])\n",
    "    PQ_max = np.array([np.max(x[\"PQ\"]) for x in PQRST])\n",
    "\n",
    "    # ST\n",
    "    ST_mean = np.array([np.mean(x[\"ST\"]) for x in PQRST])\n",
    "    ST_median = np.array([np.median(x[\"ST\"]) for x in PQRST])\n",
    "    ST_std = np.array([np.std(x[\"ST\"]) for x in PQRST])\n",
    "    ST_min = np.array([np.min(x[\"ST\"]) for x in PQRST])\n",
    "    ST_max = np.array([np.max(x[\"ST\"]) for x in PQRST])\n",
    "\n",
    "    # QT\n",
    "    QT_mean = np.array([np.mean(x[\"QT\"]) for x in PQRST])\n",
    "    QT_median = np.array([np.median(x[\"QT\"]) for x in PQRST])\n",
    "    QT_std = np.array([np.std(x[\"QT\"]) for x in PQRST])\n",
    "    QT_min = np.array([np.min(x[\"QT\"]) for x in PQRST])\n",
    "    QT_max = np.array([np.max(x[\"QT\"]) for x in PQRST])\n",
    "\n",
    "    # RS斜率\n",
    "    RS_slope_mean, RS_slope_std, RS_slope_max, RS_slope_min = get_PQRST_slope_stats(templates_lst, [sample[\"R_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "    ST_slope_mean, ST_slope_std, ST_slope_max, ST_slope_min = get_PQRST_slope_stats(templates_lst, [sample[\"S_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "\n",
    "    # QRS幅度\n",
    "    QRS_scope_mean, QRS_scope_std, QRS_scope_max, QRS_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"Q_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "\n",
    "    # PR幅度\n",
    "    PR_scope_mean, PR_scope_std, PR_scope_max, PR_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"P_id\"] for sample in PQRST], [sample[\"R_id\"] for sample in PQRST])\n",
    "\n",
    "    # QT幅度\n",
    "    QT_scope_mean, QT_scope_std, QT_scope_max, QT_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"Q_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "\n",
    "    # QR幅度\n",
    "    QR_scope_mean, QR_scope_std, QR_scope_max, QR_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"Q_id\"] for sample in PQRST], [sample[\"R_id\"] for sample in PQRST])\n",
    "    QR_range_mean, QR_range_std, QR_range_max, QR_range_min = get_PQRST_range_stats([sample[\"Q_id\"] for sample in PQRST], [sample[\"R_id\"] for sample in PQRST])\n",
    "\n",
    "    # RS幅度\n",
    "    RS_scope_mean, RS_scope_std, RS_scope_max, RS_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"R_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "    RS_range_mean, RS_range_std, RS_range_max, RS_range_min = get_PQRST_range_stats([sample[\"R_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "\n",
    "    # ST幅度\n",
    "    ST_scope_mean, ST_scope_std, ST_scope_max, ST_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"S_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "\n",
    "    # PQ幅度\n",
    "    PQ_scope_mean, PQ_scope_std, PQ_scope_max, PQ_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"P_id\"] for sample in PQRST], [sample[\"Q_id\"] for sample in PQRST])\n",
    "\n",
    "    # QS幅度\n",
    "    QS_scope_mean, QS_scope_std, QS_scope_max, QS_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"Q_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "    QS_range_mean, QS_range_std, QS_range_max, QS_range_min = get_PQRST_range_stats([sample[\"Q_id\"] for sample in PQRST], [sample[\"S_id\"] for sample in PQRST])\n",
    "\n",
    "    # RT幅度\n",
    "    RT_scope_mean, RT_scope_std, RT_scope_max, RT_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"R_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "    RT_range_mean, RT_range_std, RT_range_max, RT_range_min = get_PQRST_range_stats([sample[\"R_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "\n",
    "    # ST幅度\n",
    "    ST_scope_mean, ST_scope_std, ST_scope_max, ST_scope_min = get_PQRST_scope_stats(templates_lst, [sample[\"S_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "    ST_range_mean, ST_range_std, ST_range_max, ST_range_min = get_PQRST_range_stats([sample[\"S_id\"] for sample in PQRST], [sample[\"T_id\"] for sample in PQRST])\n",
    "\n",
    "    # error_state\n",
    "    error_count = np.array([np.sum(x[\"error_state\"]) for x in PQRST])\n",
    "    error_mean = np.array([np.mean(x[\"error_state\"]) for x in PQRST])\n",
    "\n",
    "    # 计算前2000的peak数量\n",
    "    peak_counts = np.array([(rpeak <= 2000).sum() for rpeak in rpeaks_ids])\n",
    "\n",
    "    # 计算peak的mean和std(相对，除以该ecg的最大值)\n",
    "    ecg_peak_mean = np.array([np.mean(filtered_array[rpeaks_id] / np.max(filtered_array)) for rpeaks_id, filtered_array in zip(rpeaks_ids, filtered_ecg)])\n",
    "    ecg_peak_std = np.array([np.std(filtered_array[rpeaks_id] / np.max(filtered_array)) for rpeaks_id, filtered_array in zip(rpeaks_ids, filtered_ecg)])\n",
    "    \n",
    "    # 计算ecg的绝对最大值\n",
    "    ecg_max = np.array([np.max(filtered_array) for filtered_array in filtered_ecg])\n",
    "    ecg_min = np.array([np.min(filtered_array) for filtered_array in filtered_ecg])\n",
    "    ecg_mean = np.array([np.mean(filtered_array) for filtered_array in filtered_ecg])\n",
    "    ecg_std = np.array([np.std(filtered_array) for filtered_array in filtered_ecg])\n",
    "    ecg_median = np.array([np.median(filtered_array) for filtered_array in filtered_ecg])\n",
    "\n",
    "    # 计算ecg的绝对最大最小值之比\n",
    "    ecg_max_min_ratio = np.abs(ecg_max / ecg_min)\n",
    "\n",
    "    # 计算锯齿数量\n",
    "    saw_counts = []\n",
    "    for filtered_array in filtered_ecg:\n",
    "        saw_count = 0\n",
    "        for y_1, y_2, y_3 in zip(filtered_array[:-2], filtered_array[1: -1], filtered_array[2:]):\n",
    "            if (y_1 >= y_2) != (y_2 >= y_3):\n",
    "                saw_count += 1\n",
    "        saw_counts.append(saw_count)\n",
    "    saw_counts = np.array(saw_counts)\n",
    "\n",
    "    # 截取前2000的数据中穿过0.5 Max直线的点的个数\n",
    "    upper_quant_counts = []\n",
    "    half_counts = []\n",
    "    lower_quant_counts = []\n",
    "    zero_quant_counts = []\n",
    "    neg_lower_quant_counts = []\n",
    "    neg_half_counts = []\n",
    "    neg_higher_quant_counts = []\n",
    "    percent_1_counts = []\n",
    "    percent_10_counts = []\n",
    "    percent_90_counts = []\n",
    "    percent_95_counts = []\n",
    "    percent_99_counts = []\n",
    "    neg_percent_1_counts = []\n",
    "    neg_percent_10_counts = []\n",
    "    neg_percent_90_counts = []\n",
    "    neg_percent_95_counts = []\n",
    "    neg_percent_99_counts = []\n",
    "    percent_1_99_ranges = []\n",
    "    percent_5_95_ranges = []\n",
    "    percent_10_90_ranges = []\n",
    "    percent_25_75_ranges = []\n",
    "    neg_percent_1_99_ranges = []\n",
    "    neg_percent_5_95_ranges = []\n",
    "    neg_percent_10_90_ranges = []\n",
    "    neg_percent_25_75_ranges = []\n",
    "    for filtered_array in filtered_ecg:\n",
    "        ecg_array_max = np.max(filtered_array)\n",
    "        ecg_array_min = np.min(filtered_array)\n",
    "        filtered_array = filtered_array[:2000]\n",
    "        upper_quant_count = 0\n",
    "        half_count = 0\n",
    "        lower_quant_count = 0\n",
    "        zero_quant_count = 0\n",
    "        neg_lower_quant_count = 0\n",
    "        neg_half_count = 0\n",
    "        neg_higher_quant_count = 0\n",
    "        percent_1_count = 0\n",
    "        percent_10_count = 0\n",
    "        percent_90_count = 0\n",
    "        percent_95_count = 0\n",
    "        percent_99_count = 0\n",
    "        neg_percent_1_count = 0\n",
    "        neg_percent_10_count = 0\n",
    "        neg_percent_90_count = 0\n",
    "        neg_percent_95_count = 0\n",
    "        neg_percent_99_count = 0\n",
    "        for y_1, y_2 in zip(filtered_array[:-1], filtered_array[1:]):\n",
    "            if (y_1 >= 0.5 * ecg_array_max) != (y_2 >= 0.5 * ecg_array_max):\n",
    "                half_count += 1\n",
    "            if (y_1 >= 0.75 * ecg_array_max) != (y_2 >= 0.75 * ecg_array_max):\n",
    "                upper_quant_count += 1\n",
    "            if (y_1 >= 0.25 * ecg_array_max) != (y_2 >= 0.25 * ecg_array_max):\n",
    "                lower_quant_count += 1\n",
    "            if (y_1 >= 0) + (y_2 >= 0) == 1:\n",
    "                zero_quant_count += 1\n",
    "            if (y_1 >= 0.25 * ecg_array_min) != (y_2 >= 0.25 * ecg_array_min):\n",
    "                neg_lower_quant_count += 1\n",
    "            if (y_1 >= 0.5 * ecg_array_min) != (y_2 >= 0.5 * ecg_array_min):\n",
    "                neg_half_count += 1\n",
    "            if (y_1 >= 0.75 * ecg_array_min) != (y_2 >= 0.75 * ecg_array_min):\n",
    "                neg_higher_quant_count += 1\n",
    "            if (y_1 >= 0.01 * ecg_array_max) != (y_2 >= 0.01 * ecg_array_max):\n",
    "                percent_1_count += 1\n",
    "            if (y_1 >= 0.10 * ecg_array_max) != (y_2 >= 0.10 * ecg_array_max):\n",
    "                percent_10_count += 1\n",
    "            if (y_1 >= 0.90 * ecg_array_max) != (y_2 >= 0.90 * ecg_array_max):\n",
    "                percent_90_count += 1\n",
    "            if (y_1 >= 0.95 * ecg_array_max) != (y_2 >= 0.95 * ecg_array_max):\n",
    "                percent_95_count += 1\n",
    "            if (y_1 >= 0.99 * ecg_array_max) != (y_2 >= 0.99 * ecg_array_max):\n",
    "                percent_99_count += 1\n",
    "            if (y_1 >= 0.01 * ecg_array_min) != (y_2 >= 0.01 * ecg_array_min):\n",
    "                neg_percent_1_count += 1\n",
    "            if (y_1 >= 0.10 * ecg_array_min) != (y_2 >= 0.10 * ecg_array_min):\n",
    "                neg_percent_10_count += 1\n",
    "            if (y_1 >= 0.90 * ecg_array_min) != (y_2 >= 0.90 * ecg_array_min):\n",
    "                neg_percent_90_count += 1\n",
    "            if (y_1 >= 0.95 * ecg_array_min) != (y_2 >= 0.95 * ecg_array_min):\n",
    "                neg_percent_95_count += 1\n",
    "            if (y_1 >= 0.99 * ecg_array_min) != (y_2 >= 0.99 * ecg_array_min):\n",
    "                neg_percent_99_count += 1\n",
    "        # 计算区间\n",
    "        percent_1_99_range = ((filtered_array >= 0.01 * ecg_array_max) & (filtered_array <= 0.99 * ecg_array_max)).sum()\n",
    "        percent_5_95_range = ((filtered_array >= 0.05 * ecg_array_max) & (filtered_array <= 0.95 * ecg_array_max)).sum()\n",
    "        percent_10_90_range = ((filtered_array >= 0.1 * ecg_array_max) & (filtered_array <= 0.9 * ecg_array_max)).sum()\n",
    "        percent_25_75_range = ((filtered_array >= 0.25 * ecg_array_max) & (filtered_array <= 0.75 * ecg_array_max)).sum()\n",
    "        neg_percent_1_99_range = ((filtered_array <= 0.01 * ecg_array_min) & (filtered_array >= 0.99 * ecg_array_min)).sum()\n",
    "        neg_percent_5_95_range = ((filtered_array <= 0.05 * ecg_array_min) & (filtered_array >= 0.95 * ecg_array_min)).sum()\n",
    "        neg_percent_10_90_range = ((filtered_array <= 0.1 * ecg_array_min) & (filtered_array >= 0.9 * ecg_array_min)).sum()\n",
    "        neg_percent_25_75_range = ((filtered_array <= 0.25 * ecg_array_min) & (filtered_array >= 0.75 * ecg_array_min)).sum()\n",
    "\n",
    "        upper_quant_counts.append(upper_quant_count)\n",
    "        half_counts.append(half_count)\n",
    "        lower_quant_counts.append(lower_quant_count)\n",
    "        zero_quant_counts.append(zero_quant_count)\n",
    "        neg_lower_quant_counts.append(neg_lower_quant_count)\n",
    "        neg_half_counts.append(neg_half_count)\n",
    "        neg_higher_quant_counts.append(neg_higher_quant_count)\n",
    "        percent_1_counts.append(percent_1_count)\n",
    "        percent_10_counts.append(percent_10_count)\n",
    "        percent_90_counts.append(percent_90_count)\n",
    "        percent_95_counts.append(percent_95_count)\n",
    "        percent_99_counts.append(percent_99_count)\n",
    "        neg_percent_1_counts.append(neg_percent_1_count)\n",
    "        neg_percent_10_counts.append(neg_percent_10_count)\n",
    "        neg_percent_90_counts.append(neg_percent_90_count)\n",
    "        neg_percent_95_counts.append(neg_percent_95_count)\n",
    "        neg_percent_99_counts.append(neg_percent_99_count)\n",
    "        percent_1_99_ranges.append(percent_1_99_range)\n",
    "        percent_5_95_ranges.append(percent_5_95_range)\n",
    "        percent_10_90_ranges.append(percent_10_90_range)\n",
    "        percent_25_75_ranges.append(percent_25_75_range)\n",
    "        neg_percent_1_99_ranges.append(neg_percent_1_99_range)\n",
    "        neg_percent_5_95_ranges.append(neg_percent_5_95_range)\n",
    "        neg_percent_10_90_ranges.append(neg_percent_10_90_range)\n",
    "        neg_percent_25_75_ranges.append(neg_percent_25_75_range)\n",
    "    upper_quant_counts = np.array(upper_quant_counts)\n",
    "    half_counts = np.array(half_counts)\n",
    "    lower_quant_counts = np.array(lower_quant_counts)\n",
    "    zero_quant_counts = np.array(zero_quant_counts)\n",
    "    neg_lower_quant_counts = np.array(neg_lower_quant_counts)\n",
    "    neg_half_counts = np.array(neg_half_counts)\n",
    "    neg_higher_quant_counts = np.array(neg_higher_quant_counts)\n",
    "    percent_1_counts = np.array(percent_1_counts)\n",
    "    percent_10_counts = np.array(percent_10_counts)\n",
    "    percent_90_counts = np.array(percent_90_counts)\n",
    "    percent_95_counts = np.array(percent_95_counts)\n",
    "    percent_99_counts = np.array(percent_99_counts)\n",
    "    neg_percent_1_counts = np.array(neg_percent_1_counts)\n",
    "    neg_percent_10_counts = np.array(neg_percent_10_counts)\n",
    "    neg_percent_90_counts = np.array(neg_percent_90_counts)\n",
    "    neg_percent_95_counts = np.array(neg_percent_95_counts)\n",
    "    neg_percent_99_counts = np.array(neg_percent_99_counts)\n",
    "    percent_1_99_ranges = np.array(percent_1_99_ranges)\n",
    "    percent_5_95_ranges = np.array(percent_5_95_ranges)\n",
    "    percent_10_90_ranges = np.array(percent_10_90_ranges)\n",
    "    percent_25_75_ranges = np.array(percent_25_75_ranges)\n",
    "    neg_percent_1_99_ranges = np.array(neg_percent_1_99_ranges)\n",
    "    neg_percent_5_95_ranges = np.array(neg_percent_5_95_ranges)\n",
    "    neg_percent_10_90_ranges = np.array(neg_percent_10_90_ranges)\n",
    "    neg_percent_25_75_ranges = np.array(neg_percent_25_75_ranges)\n",
    "\n",
    "    # 计算前2000的数据std\n",
    "    x_std = np.array([np.std(filtered_x[:2000]) for filtered_x in filtered_ecg])\n",
    "\n",
    "    # 计算前2000的数据的绝对差距\n",
    "    x_diff = []\n",
    "    for filtered_x in filtered_ecg:\n",
    "        filtered_x = filtered_x[:2000] / np.max(filtered_x[:2000])\n",
    "        filtered_x_right = filtered_x[1:]\n",
    "        filtered_x_left = filtered_x[:-1]\n",
    "        filtered_x_diff = np.sum(np.abs(filtered_x_right - filtered_x_left))\n",
    "        x_diff.append(filtered_x_diff)\n",
    "    x_diff = np.array(x_diff)\n",
    "\n",
    "    # valid features\n",
    "    valid_features = [\n",
    "        # rpeak\n",
    "        rpeak_mean, \n",
    "        rpeak_median, \n",
    "        rpeak_std, \n",
    "        rpeak_min, \n",
    "        rpeak_max, \n",
    "        # heart rate\n",
    "        hr_mean, \n",
    "        hr_median, \n",
    "        hr_std, \n",
    "        hr_min, \n",
    "        hr_max, \n",
    "        # P\n",
    "        P_mean, \n",
    "        P_median, \n",
    "        P_std, \n",
    "        P_min, \n",
    "        P_max, \n",
    "        # Q\n",
    "        Q_mean, \n",
    "        Q_median, \n",
    "        Q_std, \n",
    "        Q_min, \n",
    "        Q_max, \n",
    "        # R\n",
    "        R_mean, \n",
    "        R_median, \n",
    "        R_std, \n",
    "        R_min, \n",
    "        R_max, \n",
    "        # S\n",
    "        S_mean, \n",
    "        S_median, \n",
    "        S_std, \n",
    "        S_min, \n",
    "        S_max, \n",
    "        # T\n",
    "        T_mean, \n",
    "        T_median, \n",
    "        T_std, \n",
    "        T_min, \n",
    "        T_max, \n",
    "        # P_id\n",
    "        P_id_mean, \n",
    "        P_id_median, \n",
    "        P_id_std, \n",
    "        P_id_min, \n",
    "        P_id_max, \n",
    "        # Q_id\n",
    "        Q_id_mean, \n",
    "        Q_id_median, \n",
    "        Q_id_std, \n",
    "        Q_id_min, \n",
    "        Q_id_max, \n",
    "        # R_id\n",
    "        R_id_mean, \n",
    "        R_id_median, \n",
    "        R_id_std, \n",
    "        R_id_min, \n",
    "        R_id_max, \n",
    "        # S_id\n",
    "        S_id_mean, \n",
    "        S_id_median, \n",
    "        S_id_std, \n",
    "        S_id_min, \n",
    "        S_id_max, \n",
    "        # T_id\n",
    "        T_id_mean, \n",
    "        T_id_median, \n",
    "        T_id_std, \n",
    "        T_id_min, \n",
    "        T_id_max, \n",
    "        # QRS\n",
    "        QRS_mean, \n",
    "        QRS_median, \n",
    "        QRS_std, \n",
    "        QRS_min, \n",
    "        QRS_max, \n",
    "        # PR\n",
    "        PR_mean, \n",
    "        PR_median, \n",
    "        PR_std, \n",
    "        PR_min, \n",
    "        PR_max, \n",
    "        # PQ\n",
    "        PQ_mean, \n",
    "        PQ_median, \n",
    "        PQ_std, \n",
    "        PQ_min, \n",
    "        PQ_max, \n",
    "        # ST\n",
    "        ST_mean, \n",
    "        ST_median, \n",
    "        ST_std, \n",
    "        ST_min, \n",
    "        ST_max, \n",
    "        # QT\n",
    "        QT_mean, \n",
    "        QT_median, \n",
    "        QT_std, \n",
    "        QT_min, \n",
    "        QT_max, \n",
    "        # PQRST范围和幅度\n",
    "        QRS_scope_mean, QRS_scope_std, QRS_scope_max, QRS_scope_min, \n",
    "        PR_scope_mean, PR_scope_std, PR_scope_max, PR_scope_min, \n",
    "        QT_scope_mean, QT_scope_std, QT_scope_max, QT_scope_min,\n",
    "        QR_scope_mean, QR_scope_std, QR_scope_max, QR_scope_min,\n",
    "        QR_range_mean, QR_range_std, QR_range_max, QR_range_min,\n",
    "        RS_scope_mean, RS_scope_std, RS_scope_max, RS_scope_min,\n",
    "        RS_range_mean, RS_range_std, RS_range_max, RS_range_min,\n",
    "        ST_scope_mean, ST_scope_std, ST_scope_max, ST_scope_min,\n",
    "        PQ_scope_mean, PQ_scope_std, PQ_scope_max, PQ_scope_min,\n",
    "        QS_scope_mean, QS_scope_std, QS_scope_max, QS_scope_min,\n",
    "        QS_range_mean, QS_range_std, QS_range_max, QS_range_min,\n",
    "        RT_scope_mean, RT_scope_std, RT_scope_max, RT_scope_min,\n",
    "        RT_range_mean, RT_range_std, RT_range_max, RT_range_min,\n",
    "        ST_scope_mean, ST_scope_std, ST_scope_max, ST_scope_min,\n",
    "        ST_range_mean, ST_range_std, ST_range_max, ST_range_min,\n",
    "        # RS和ST斜率\n",
    "        RS_slope_mean, RS_slope_std, RS_slope_max, RS_slope_min, \n",
    "        ST_slope_mean, ST_slope_std, ST_slope_max, ST_slope_min,\n",
    "        # error state\n",
    "        error_count, \n",
    "        error_mean, \n",
    "        # 前2000的peak数量\n",
    "        peak_counts, \n",
    "        # 前2000的数据std\n",
    "        x_std, \n",
    "        # 计算前2000的数据的绝对差距\n",
    "        x_diff, \n",
    "\n",
    "        # 计算peak的mean和std(相对，除以该ecg的最大值)\n",
    "        ecg_peak_mean, \n",
    "        ecg_peak_std, \n",
    "        ecg_max, \n",
    "        ecg_min, \n",
    "        ecg_mean, \n",
    "        ecg_std, \n",
    "        ecg_median, \n",
    "        ecg_max_min_ratio, \n",
    "\n",
    "        # 锯齿数量\n",
    "        saw_counts, \n",
    "\n",
    "        # 截取前2000的数据中穿过某个阈值的直线的点的个数\n",
    "        upper_quant_counts, \n",
    "        half_counts,\n",
    "        lower_quant_counts, \n",
    "        zero_quant_counts,\n",
    "        neg_lower_quant_counts,\n",
    "        neg_half_counts,\n",
    "        neg_higher_quant_counts, \n",
    "        percent_1_counts, \n",
    "        percent_10_counts, \n",
    "        percent_90_counts, \n",
    "        percent_95_counts, \n",
    "        percent_99_counts, \n",
    "        neg_percent_1_counts, \n",
    "        neg_percent_10_counts, \n",
    "        neg_percent_90_counts,\n",
    "        neg_percent_95_counts,\n",
    "        neg_percent_99_counts, \n",
    "        percent_1_99_ranges, \n",
    "        percent_5_95_ranges, \n",
    "        percent_1_99_ranges, \n",
    "        percent_5_95_ranges, \n",
    "        percent_10_90_ranges, \n",
    "        percent_25_75_ranges, \n",
    "        neg_percent_1_99_ranges, \n",
    "        neg_percent_5_95_ranges, \n",
    "        neg_percent_10_90_ranges, \n",
    "        neg_percent_25_75_ranges, \n",
    "    ]\n",
    "    return valid_features\n",
    "\n",
    "valid_features = get_valid_features(rpeaks_new, heart_rate_new, PQRST, rpeaks_lst, filtered_lst, templates_lst)\n",
    "valid_features_test = get_valid_features(rpeaks_new_test, heart_rate_new_test, PQRST_test, rpeaks_lst_test, filtered_lst_test, templates_lst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成新的训练数据 X_train_features\n",
    "X_train_features = []\n",
    "for feature in valid_features:\n",
    "    feature = feature.reshape((X_train.shape[0], -1))\n",
    "    X_train_features.append(feature)\n",
    "X_train_features = np.concatenate(X_train_features, axis=1)\n",
    "\n",
    "X_test_features = []\n",
    "for feature in valid_features_test:\n",
    "    feature = feature.reshape((X_test.shape[0], -1))\n",
    "    X_test_features.append(feature)\n",
    "X_test_features = np.concatenate(X_test_features, axis=1)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_features = feature_scaler.fit_transform(X_train_features)\n",
    "X_test_features = feature_scaler.transform(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/X_train_features.npy\", X_train_features)\n",
    "np.save(\"Data/X_test_features.npy\", X_test_features)\n",
    "np.save(\"Data/y_train.npy\", y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
